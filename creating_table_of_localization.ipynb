{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d7cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# from hloc import extract_features, match_features, reconstruction, visualization\n",
    "# from hloc import colmap_for_habitat, triangulation, localize_sfm, visualization\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pycolmap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import g2o\n",
    "from enum import Enum\n",
    "import cv2\n",
    "# from threading import Lock\n",
    "# from hloc.utils.parsers import (\n",
    "#     parse_image_lists_with_intrinsics, parse_retrieval, names_to_pair)\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from time import perf_counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af440a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path('/datasets/Habitat/')\n",
    "images_path = dataset / 'extracted_HPointLoc'\n",
    "\n",
    "# outputs = Path('/datasets/Habitat/Hierarchical_Localization_outputs/sfm/') \n",
    "# loc_pairs = dataset / 'FBoW/scores_FBoW_all_maps.txt'  # top 50 retrieved by NetVLAD\n",
    "\n",
    "# feature_filename = f\"{features}.h5\"\n",
    "# match_filename = f\"{features}_{matcher_conf['output']}_{loc_pairs.stem}.h5\"\n",
    "\n",
    "path_to_hdf5_datasets = '/datasets/Habitat/HPointLoc/1LXtFkjw3qL_point0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444e19f",
   "metadata": {},
   "source": [
    "# Getting table of localizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c970414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_rotation_matrix(quaternion_wxyz):\n",
    "    r = R.from_quat([quaternion_wxyz[1], quaternion_wxyz[2], quaternion_wxyz[3], quaternion_wxyz[0]])\n",
    "    matrix = r.as_matrix()\n",
    "    matrix[:3,2] = -matrix[:3,2]\n",
    "    matrix[:3,1] = -matrix[:3,1]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aca934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_3d(x, y, depth, fx, fy, cx, cy, cam_center_world, R_world_to_cam, w_in_quat_first = True):\n",
    "    if depth <= 0:\n",
    "        return 0\n",
    "    new_x = (x - cx)*depth/fx\n",
    "    new_y = (y - cy)*depth/fy\n",
    "    new_z = depth\n",
    "    coord_3D_world_to_cam = np.array([new_x, new_y, new_z], float)\n",
    "    if len(R_world_to_cam) == 4:\n",
    "        if w_in_quat_first:\n",
    "            matrix = quaternion_to_rotation_matrix(R_world_to_cam)\n",
    "        else:\n",
    "            R_world_to_cam = [R_world_to_cam[3], R_world_to_cam[0], R_world_to_cam[1], R_world_to_cam[2]]\n",
    "            matrix = quaternion_to_rotation_matrix(R_world_to_cam)\n",
    "    coord_3D_cam_to_world = np.matmul(matrix, coord_3D_world_to_cam) + cam_center_world\n",
    "    return coord_3D_cam_to_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3977f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.array([0, 0.25, 0.5, 1, 2, 3, 4, 5, 10, 20, 30, 50])\n",
    "angles = np.array([0, 1, 2, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 120, 150, 180], dtype=float)\n",
    "\n",
    "table = {}\n",
    "for distance in distances:\n",
    "    if distance == 0:\n",
    "        continue\n",
    "    for angle in angles:\n",
    "        if angle == 0:\n",
    "            continue\n",
    "        table[str(distance)+\" m and \"+ str(angle) + \" deg\"] = {\n",
    "            'tp'           : 0,\n",
    "            'all'           : 0,\n",
    "            'queries-dbs' : {}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99c263",
   "metadata": {},
   "source": [
    "## Creating empty table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = BundleAdjustment()\n",
    "\n",
    "for filename1 in tqdm(sorted(os.listdir(path_to_images))):\n",
    "    query1_filename = filename1.rstrip('.png')\n",
    "    hdf5_filename_query1 = '_'.join(query1_filename.split('_')[:2]) + '.hdf5'\n",
    "    hdf5_file_query1 = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query1), 'r')\n",
    "    num_query1 = int(query1_filename.split('_')[-1])\n",
    "    \n",
    "    is_database = ''\n",
    "    if query1_filename.find('database') != -1:\n",
    "        is_database = '_base'\n",
    "    \n",
    "    depth_query1 = 10*hdf5_file_query1['depth'+is_database][num_query1]\n",
    "    \n",
    "    translation = hdf5_file_query1['gps'+is_database][num_query1]\n",
    "    orientation = quaternion_to_rotation_matrix(hdf5_file_query1['quat'+is_database][num_query1])\n",
    "    pose_44_query1 = np.eye(4)\n",
    "    pose_44_query1[:3,:3] = orientation\n",
    "    pose_44_query1[:3,3] = translation\n",
    "    \n",
    "    # Заполняем таблицу метрик    \n",
    "    for filename2 in sorted(os.listdir(path_to_images)):\n",
    "        if filename1 == filename2:\n",
    "            continue\n",
    "        query2_filename = filename2.rstrip('.png')\n",
    "        hdf5_filename_query2 = '_'.join(query2_filename.split('_')[:2]) + '.hdf5'\n",
    "        hdf5_file_query2 = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query2), 'r')\n",
    "        \n",
    "        num_query2 = int(query2_filename.split('_')[-1])\n",
    "        \n",
    "        is_database = ''\n",
    "        if query2_filename.find('database') != -1:\n",
    "            is_database = '_base'\n",
    "        \n",
    "        translation = hdf5_file_query2['gps'+is_database][num_query2]\n",
    "        orientation = quaternion_to_rotation_matrix(hdf5_file_query2['quat'+is_database][num_query2])\n",
    "        pose_44_query2 = np.eye(4)\n",
    "        pose_44_query2[:3,:3] = orientation\n",
    "        pose_44_query2[:3,3] = translation\n",
    "        \n",
    "        diff_pose = np.linalg.inv(pose_44_query2) @ pose_44_query1\n",
    "        dist_diff = np.sum(diff_pose[:3, 3]**2) ** 0.5\n",
    "        r = R.from_matrix(diff_pose[:3, :3])\n",
    "        rotvec = r.as_rotvec()\n",
    "        angle_diff = (np.sum(rotvec**2)**0.5) * 180 / 3.14159265353\n",
    "        angle_diff = abs(180 - abs(angle_diff-180))\n",
    "        \n",
    "        num_dist =  [i for i in range(1, len(distances)-1) \\\n",
    "                     if (distances[i-1] < dist_diff) and (distances[i] >= dist_diff)]\n",
    "        \n",
    "        num_angle = [i for i in range(1, len(angles)-1) \\\n",
    "                     if (angles[i-1] < angle_diff) and (angles[i] >= angle_diff)]\n",
    "        \n",
    "        # Если не входит в таблицу, то пропускаем эту пару\n",
    "        if len(num_dist) == 0 or len(num_angle) == 0:\n",
    "            continue\n",
    "        \n",
    "        if query1_filename in table[str(distances[num_dist[0]])+\" m and \" + str(angles[num_angle[0]])+\" deg\"]['queries-dbs'].keys():\n",
    "            table[str(distances[num_dist[0]])+\" m and \" + str(angles[num_angle[0]])+\" deg\"]['queries-dbs'][query1_filename][\"db_filenames\"].append(query2_filename)\n",
    "#             table[str(distances[num_dist[0]])+\" m and \" + str(angles[num_angle[0]])+\" deg\"][\"all\"] += 1\n",
    "        else:\n",
    "            table[str(distances[num_dist[0]])+\" m and \" + str(angles[num_angle[0]])+\" deg\"]['queries-dbs'][query1_filename] = {\n",
    "                \"db_filenames\"   : [query2_filename]\n",
    "            }\n",
    "#             table[str(distances[num_dist[0]])+\" m and \" + str(angles[num_angle[0]])+\" deg\"][\"all\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579e67d",
   "metadata": {},
   "source": [
    "## Filling table with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b498047",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_keypoints = 4096\n",
    "path_to_global_descriptors = '/datasets/Habitat/HF_Net_output/global_descriptors'\n",
    "\n",
    "# conf = confs['superglue']\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Model = dynamic_load(matchers, conf['model']['name'])\n",
    "# model = Model(conf['model']).eval().to(device)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for cell in tqdm(table.keys()):\n",
    "    if len(table[cell]['queries-dbs'].keys()) == 0:\n",
    "        continue\n",
    "        \n",
    "#     if cell != \"2.0 m and 10.0 deg\":\n",
    "#         continue\n",
    "        \n",
    "    for query_image in table[cell]['queries-dbs'].keys():\n",
    "        \n",
    "        # RUN global descriptor retrieval and find top-1 retrieval image\n",
    "    \n",
    "        dbFeat = []\n",
    "\n",
    "        with open(os.path.join(path_to_global_descriptors, query_image+'.npy'), 'rb') as f:\n",
    "            descriptor_query = np.load(f)\n",
    "        qFeat = np.empty((1, num_keypoints))\n",
    "        qFeat[0,:] = descriptor_query\n",
    "        qFeat = qFeat.astype('float32')\n",
    "        faiss_index = faiss.IndexFlatL2(num_keypoints)\n",
    "        faiss_index.add(qFeat)\n",
    "    \n",
    "#         measurements = []\n",
    "#         optimizer.clear()\n",
    "        for db_image in table[cell]['queries-dbs'][query_image][\"db_filenames\"]:\n",
    "        \n",
    "            with open(os.path.join(path_to_global_descriptors, db_image+'.npy'), 'rb') as f:\n",
    "                descriptor2 = np.load(f)\n",
    "            dbFeat.append(descriptor2)\n",
    "        \n",
    "        dbFeat_temp = np.empty((len(dbFeat), num_keypoints))\n",
    "\n",
    "        for i in range(len(dbFeat)):\n",
    "            dbFeat_temp[i,:] = dbFeat[i]\n",
    "    \n",
    "        dbFeat = dbFeat_temp\n",
    "    \n",
    "        dbFeat = dbFeat.astype('float32')\n",
    "        distances, predictions = faiss_index.search(dbFeat, 1)\n",
    "        num_db = predictions[0][0]\n",
    "        db_image = table[cell]['queries-dbs'][query_image][\"db_filenames\"][num_db]\n",
    "        image_1 = query_image\n",
    "        image_2 = db_image\n",
    "        \n",
    "        dbfeature = dbFeat[num_db]\n",
    "        qfeature  = qFeat[0]\n",
    "        dbfeature = np.reshape(dbfeature, (1,-1))\n",
    "        qfeature = np.reshape(qfeature, (1,-1))\n",
    "        simil_score = cosine_similarity(dbfeature, qfeature)[0][0]\n",
    "    \n",
    "#----------- The following commented is for localization results (SuperPoint + SuperGLue + g2o) ---------------\n",
    "\n",
    "#         # Run SuperPoint + SuperGlue + G2O\n",
    "    \n",
    "#         query1_filename = image_1\n",
    "#         is_database = ''\n",
    "#         if query1_filename.find('database') != -1:\n",
    "#             is_database = '_base'\n",
    "#         hdf5_filename_query1 = '_'.join(query1_filename.split('_')[:2]) + '.hdf5'\n",
    "#         hdf5_file_query1 = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query1), 'r')\n",
    "#         num_query1 = int(query1_filename.split('_')[-1])\n",
    "#         depth_query1 = 10*hdf5_file_query1['depth'+is_database][num_query1]\n",
    "#         translation = hdf5_file_query1['gps'+is_database][num_query1]\n",
    "#         orientation = quaternion_to_rotation_matrix(hdf5_file_query1['quat'+is_database][num_query1])\n",
    "#         pose_44_query1 = np.eye(4)\n",
    "#         pose_44_query1[:3,:3] = orientation\n",
    "#         pose_44_query1[:3,3] = translation\n",
    "#         keypoints_query = keypoints_file[query1_filename]['keypoints'].__array__()\n",
    "    \n",
    "#         db_filename = image_2\n",
    "#         hdf5_filename_db = '_'.join(db_filename.split('_')[:2]) + '.hdf5'\n",
    "#         hdf5_file_db = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_db), 'r')\n",
    "#         num_db = int(db_filename.split('_')[-1])\n",
    "#         is_database = ''\n",
    "#         if db_filename.find('database') != -1:\n",
    "#             is_database = '_base'\n",
    "#         translation = hdf5_file_db['gps'+is_database][num_db]\n",
    "#         orientation = quaternion_to_rotation_matrix(hdf5_file_db['quat'+is_database][num_db])\n",
    "#         pose_44_db = np.eye(4)\n",
    "#         pose_44_db[:3,:3] = orientation\n",
    "#         pose_44_db[:3,3] = translation\n",
    "#         depth_db = 10*hdf5_file_db['depth'+is_database][num_db]\n",
    "#         keypoints_db = keypoints_file[db_filename]['keypoints'].__array__()\n",
    "    \n",
    "#         pose = g2o.Isometry3d(pose_44_db)\n",
    "#         optimizer.add_pose(0, pose, cam, fixed=False)\n",
    "    \n",
    "#         retrieved_image = image_2\n",
    "#         hdf5_file_retrieved = hdf5_file_db\n",
    "#         num_retrieved = num_db\n",
    "#         depth_retrieved = depth_db\n",
    "        \n",
    "#         # Extract matches\n",
    "        \n",
    "#         data = {}\n",
    "#         feats0, feats1 = keypoints_file[query_image], keypoints_file[db_image]\n",
    "#         for k in feats1.keys():\n",
    "#             data[k+'0'] = feats0[k].__array__()\n",
    "#         for k in feats1.keys():\n",
    "#             data[k+'1'] = feats1[k].__array__()\n",
    "#         data = {k: torch.from_numpy(v)[None].float().to(device)\n",
    "#                 for k, v in data.items()}\n",
    "#         data['image0'] = torch.empty((1, 1,)+tuple(feats0['image_size'])[::-1])\n",
    "#         data['image1'] = torch.empty((1, 1,)+tuple(feats1['image_size'])[::-1])\n",
    "        \n",
    "#         pred = model(data)\n",
    "        \n",
    "#         matches_db_keypoints_ids = pred['matches0'][0].cpu().short().numpy()\n",
    "#         matched_keypoints_query_ids = np.where(matches_db_keypoints_ids > -1)[0]\n",
    "        \n",
    "#         # Пробегаемся по всем индексам кипоинтов query image, которые имеют сопоставление с database image\n",
    "#         for num_kp, keypoint_id in enumerate(matched_keypoints_query_ids):\n",
    "#             if keypoint_id == -1:\n",
    "#                 continue\n",
    "#             # затем получаем индекс сопоставленного кипоинта на database image\n",
    "#             keypoint_id_query = keypoint_id\n",
    "#             keypoint_id_database = matches_db_keypoints_ids[keypoint_id]\n",
    "                \n",
    "#             # получаем координаты x,y для сопоставленных кипоинтов на query и database\n",
    "#             keypoint_xy_query = keypoints_query[keypoint_id_query]\n",
    "#             keypoint_xy_db = keypoints_db[keypoint_id_database]\n",
    "                \n",
    "#             # получаем глубины данных кипоинтов\n",
    "#             depth_keypoint_query = depth_query1[int(keypoint_xy_query[1]), int(keypoint_xy_query[0])][0]\n",
    "#             depth_keypoint_db = depth_retrieved[int(keypoint_xy_db[1]), int(keypoint_xy_db[0])][0]\n",
    "                \n",
    "#             # если глубина кипоинтов <=0, то она скорее всего невалидна, пропускаем ее\n",
    "#             if depth_keypoint_db <= 0 or depth_keypoint_query <= 0:\n",
    "#                 continue\n",
    "                    \n",
    "#             # Получаем координаты виртуального кипоинта для виртуальной правой камеры на query image\n",
    "#             x_right = keypoint_xy_query[0] - fx_baseline/depth_keypoint_query\n",
    "                \n",
    "#             # Создаем объекты кипоинтов (на query image, left and right cam), подходящие для погрузки их в g2o\n",
    "#             kp_left, kp_right = cv2.KeyPoint(), cv2.KeyPoint()\n",
    "#             kp_left.pt  = (keypoint_xy_query[0], keypoint_xy_query[1])\n",
    "#             kp_right.pt = (x_right, keypoint_xy_query[1])\n",
    "#             meas = Measurement(Measurement.Type.STEREO,\n",
    "#                                Measurement.Source.TRIANGULATION,\n",
    "#                                [kp_left, kp_right])\n",
    "                \n",
    "#             #  Теперь нужно загрузить в g2o 3D координаты кипоинта на database image. Этот кипоинт сопоставлен\n",
    "#             # с кипоинтом на query image\n",
    "#             # Получаем 3D координату ключевой точки на database image\n",
    "#             global_xyz_of_keypoint = get_point_3d(keypoint_xy_db[0], keypoint_xy_db[1], depth_keypoint_db, \\\n",
    "#                                      fx, fy, cx, cy, \\\n",
    "#                                      translation, hdf5_file_retrieved['quat'+is_database][num_retrieved])\n",
    "#             mappoint = MapPoint(global_xyz_of_keypoint)\n",
    "#             # загружаем 3D координату кипоинта в объект meas, который будет теперь содержать координаты (x,y)\n",
    "#             # киопинта на query image и 3D координаты (x,y,z) кипоинта на database image \n",
    "#             meas.mappoint = mappoint\n",
    "#             measurements.append(meas)\n",
    "                \n",
    "#         for i, m in enumerate(measurements):\n",
    "#             optimizer.add_point(i, m.mappoint.position, fixed=True)\n",
    "#             optimizer.add_edge(0, i, 0, m)\n",
    "#         optimizer.optimize(10)\n",
    "#         result = optimizer.get_pose(0)\n",
    "    \n",
    "#         pose_estimated = np.eye(4)\n",
    "#         # Now get pose from g2o optimization \n",
    "#         matrix = result.matrix()[:3,:3]\n",
    "#         pose_estimated[:3, :3] = matrix\n",
    "#         pose_estimated[:3, 3] = result.position()\n",
    "    \n",
    "#         error_pose = np.linalg.inv(pose_estimated) @ pose_44_query1\n",
    "#         dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "#         r = R.from_matrix(error_pose[:3, :3])\n",
    "#         rotvec = r.as_rotvec()\n",
    "#         angle_error = (np.sum(rotvec**2)**0.5) * 180 / 3.14159265353\n",
    "#         angle_error = abs(180 - abs(angle_error - 180))\n",
    "    \n",
    "        table[cell]['all'] += 1\n",
    "#         if angle_error < 5 and dist_error < 0.5:\n",
    "#             table[cell]['tp'] += 1\n",
    "            \n",
    "#         table[cell]['queries-dbs'][query_image][\"estimated_pose\"] = pose_estimated.tolist()\n",
    "        table[cell]['queries-dbs'][query_image][\"retrieved_image\"] = retrieved_image\n",
    "        table[cell]['queries-dbs'][query_image][\"retrival_score\"] = simil_score\n",
    "        table[cell]['queries-dbs'][query_image][\"num_matches\"] = float(simil_score)\n",
    "            \n",
    "# ----------------- The following is for saving images and further making video -----------------------\n",
    "\n",
    "#         if dist_error > 0.5 or angle_error > 5 and save_images:\n",
    "            \n",
    "#             new_image = Image.new('RGB',(2*256, 256), (250,250,250))\n",
    "\n",
    "#             db_image1 = db_image\n",
    "        \n",
    "#             image1 = Image.open(str(images_path / Path(query_image))+'.png')\n",
    "#             image2 = Image.open(str(images_path / Path(db_image1))+'.png')\n",
    "        \n",
    "# #             metric1 = list(retrievals[query_image+'.png'][0].values())[0]\n",
    "        \n",
    "#             new_image.paste(image1,(0,0))\n",
    "#             new_image.paste(image2,(255,0))\n",
    "\n",
    "#             draw = ImageDraw.Draw(new_image)\n",
    "        \n",
    "#             draw.text((0, 0), query_image,(255,255,255), font=font)\n",
    "#             draw.text((image1.size[0], 0),\"{}\\n{}\".format(db_image.rstrip('.png'), round(float(simil_score),3)),(255,255,255),font=font)\n",
    "\n",
    "#             new_image.save('{}/{:04}.jpg'.format(output_folder_for_images, count),'JPEG')\n",
    "#             count += 1\n",
    "            \n",
    "#         print(table[cell])\n",
    "#         print(dist_error)\n",
    "#         print(angle_error)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the filled table into json\n",
    "\n",
    "import json\n",
    "with open('table.json', 'w') as outfile:\n",
    "    json.dump(table, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213a5d4",
   "metadata": {},
   "source": [
    "# Printing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5e551e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eaf6484e21c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate #pip install tabulate\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = '/home/cds-s/workspace/hierarchical_localization/table.json'\n",
    "path_to_hdf5_datasets = '/media/cds-s/data/Datasets/Habitat/1LXtFkjw3qL_point0/hdf5/'\n",
    "output_path = '/media/cds-s/data/Datasets/Habitat/table.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f889f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_data) as f:\n",
    "    table = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12079e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_rotation_matrix(qvec):\n",
    "    r = R.from_quat([qvec[1], qvec[2], qvec[3], qvec[0]])\n",
    "    result = r.as_matrix()\n",
    "    result[:3,2] = -result[:3,2]\n",
    "    result[:3,1] = -result[:3,1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a428e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_thr = 10\n",
    "dist_thr = 2\n",
    "table_metrics = {}\n",
    "for key in tqdm(table.keys()):\n",
    "    if not(key in table_metrics.keys()):\n",
    "        table_metrics[key] = {\n",
    "            'tp'  : 0,\n",
    "            'all' : 0\n",
    "        }\n",
    "    for query in table[key]['queries-dbs']:\n",
    "        hdf5_filename = '_'.join(query.split('_')[:2]) + '.hdf5'\n",
    "        num_image = int(query.split('_')[-1])\n",
    "        hdf5_file = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename), 'r')\n",
    "        pose_query_gt = np.eye(4)\n",
    "        is_database = \"\"\n",
    "        if query.find('database') != -1:\n",
    "            is_database = \"_base\"\n",
    "        pose_query_gt[:3,:3] = quaternion_to_rotation_matrix(hdf5_file['quat'+is_database][num_image])\n",
    "        pose_query_gt[:3,3] = hdf5_file['gps'+is_database][num_image]\n",
    "        pose_query_estimated = table[key]['queries-dbs'][query]['estimated_pose']\n",
    "        error_pose = np.linalg.inv(pose_query_estimated) @ pose_query_gt\n",
    "        dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "        r = R.from_matrix(error_pose[:3, :3])\n",
    "        rotvec = r.as_rotvec()\n",
    "        angle_error = (np.sum(rotvec**2)**0.5) * 180 / 3.14159265353\n",
    "        angle_error = abs(180 - abs(angle_error-180))\n",
    "        if angle_error < angle_thr and dist_error < dist_thr:\n",
    "            table_metrics[key]['tp'] += 1\n",
    "        table_metrics[key]['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6b797",
   "metadata": {},
   "source": [
    "### Printing into this jupyter notebook the whole table and into text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb708020",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "angles = []\n",
    "for key in table.keys():\n",
    "    distance, _, _, angle, _ = key.split(' ')\n",
    "    distances.append(float(distance))\n",
    "    angles.append(float(angle))\n",
    "distances = sorted(distances)\n",
    "angles = sorted(list(set(angles)))[::-1]\n",
    "\n",
    "distances = sorted(list(set(distances)))\n",
    "\n",
    "table_for_printing = []\n",
    "\n",
    "set_of_existing_angles = set()\n",
    "\n",
    "num_angle = 0\n",
    "num_distance = 0\n",
    "stroka = []\n",
    "\n",
    "output_file = open(output_path, 'w')\n",
    "while num_angle != len(angles):\n",
    "    for key in table.keys():\n",
    "        distance, _, _, angle, _ = key.split(' ')\n",
    "        distance = float(distance)\n",
    "        angle = float(angle)\n",
    "        if angle == angles[num_angle] and distance == distances[num_distance]:\n",
    "            if num_distance == 0:\n",
    "                if num_angle != len(angles)-1:\n",
    "                    stroka = [str(int(angles[num_angle+1])) + \"°-\\n\"+ str(int(angles[num_angle]))+\"°\"]\n",
    "                else:\n",
    "                    stroka = [\"0°-\\n\"+ str(int(angles[num_angle])) + \" °\"]\n",
    "                stroka.append(str(table[key][\"tp\"])+\"/\"+str(table[key][\"all\"]))\n",
    "                num_distance += 1\n",
    "            else:\n",
    "                num_distance += 1\n",
    "                stroka.append(str(table[key][\"tp\"])+\"/\"+str(table[key][\"all\"]))\n",
    "            if num_distance == len(distances):\n",
    "                stroka_for_txt_file = stroka[1:]\n",
    "                stroka_for_txt_file = ' '.join(stroka_for_txt_file)\n",
    "                output_file.write(stroka_for_txt_file + \"\\n\")\n",
    "                table_for_printing.append(stroka)\n",
    "                num_distance = 0\n",
    "                num_angle += 1\n",
    "                if num_angle == len(angles):\n",
    "                    break\n",
    "\n",
    "# print(stroka)\n",
    "\n",
    "table_for_printing.append([\" \"] + ['0-\\n'+str(distances[0])+\" m\"] + [str(distances[i])+\"-\\n\"+str(distance)+\" m\" for i, distance in enumerate(distances[1:])])\n",
    "\n",
    "print(tabulate(table_for_printing, tablefmt='fancy_grid'))\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847dcff",
   "metadata": {},
   "source": [
    "### The same but without printing into jupyter notebook, only into text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666aae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "angles = []\n",
    "for key in table.keys():\n",
    "    distance, _, _, angle, _ = key.split(' ')\n",
    "    distances.append(float(distance))\n",
    "    angles.append(float(angle))\n",
    "distances = sorted(distances)\n",
    "angles = sorted(list(set(angles)))[::-1]\n",
    "\n",
    "distances = sorted(list(set(distances)))\n",
    "\n",
    "table_for_printing = []\n",
    "\n",
    "set_of_existing_angles = set()\n",
    "\n",
    "num_angle = 0\n",
    "num_distance = 0\n",
    "stroka = []\n",
    "\n",
    "output_file = open(output_path, 'w')\n",
    "while num_angle != len(angles):\n",
    "    for key in table.keys():\n",
    "        distance, _, _, angle, _ = key.split(' ')\n",
    "        distance = float(distance)\n",
    "        angle = float(angle)\n",
    "        if angle == angles[num_angle] and distance == distances[num_distance]:\n",
    "            num_distance += 1\n",
    "            if table[key][\"all\"] != 0:\n",
    "                stroka.append(str(table[key][\"tp\"]/table[key][\"all\"]))\n",
    "            else:\n",
    "                stroka.append(str(0))\n",
    "            if num_distance == len(distances):\n",
    "                stroka_for_txt_file = ' '.join(stroka)\n",
    "                stroka = []\n",
    "                output_file.write(stroka_for_txt_file + \"\\n\")\n",
    "                num_distance = 0\n",
    "                num_angle += 1\n",
    "                if num_angle == len(angles):\n",
    "                    break\n",
    "\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
