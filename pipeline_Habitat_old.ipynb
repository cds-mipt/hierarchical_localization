{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/08/2021 10:22:05 INFO] Generating new fontManager, this may take some time...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from hloc import extract_features, match_features, reconstruction, visualization\n",
    "from hloc import colmap_for_habitat, triangulation, localize_sfm, visualization\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pycolmap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import math\n",
    "import g2o\n",
    "from enum import Enum\n",
    "import cv2\n",
    "from threading import Lock\n",
    "from hloc.utils.parsers import (\n",
    "    parse_image_lists_with_intrinsics, parse_retrieval, names_to_pair)\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In this notebook, we will run SfM reconstruction from scratch on a set of images. We choose the [South-Building dataset](https://openaccess.thecvf.com/content_cvpr_2013/html/Hane_Joint_3D_Scene_2013_CVPR_paper.html) - we will download it later. First, we define some paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path('/datasets/Habitat/')\n",
    "images_path = dataset / '1LXtFkjw3qL_point0/images'\n",
    "\n",
    "outputs = Path('/datasets/Habitat/Hierarchical_Localization_outputs/sfm/')\n",
    "# sfm_pairs = outputs / 'pairs_exhaustive_base-around.txt'\n",
    "# sfm_pairs = outputs / 'pairs_exhaustive.txt'\n",
    "sfm_dir = outputs / 'sfm_superpoint+superglue'\n",
    "sfm_pairs = dataset / 'NetVLAD/NetVLAD_database_top_40.txt' \n",
    "loc_pairs = dataset / 'NetVLAD/NetVLAD_query_top_40.txt'  # top 50 retrieved by NetVLAD\n",
    "\n",
    "feature_conf = extract_features.confs['superpoint_habitat']\n",
    "matcher_conf = match_features.confs['superglue']\n",
    "\n",
    "features = feature_conf['output']\n",
    "feature_filename = f\"{features}.h5\"\n",
    "match_filename = f\"{features}_{matcher_conf['output']}_{sfm_pairs.stem}.h5\"\n",
    "reference_sfm = outputs / 'sfm_superpoint+superglue'  # the SfM model we will build\n",
    "\n",
    "results = outputs / 'Habitat_hloc_superpoint+superglue_netvlad40.txt'  # the result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx, cy = 128, 128\n",
    "fx, fy = 128, 128\n",
    "width, height = 256., 256.\n",
    "fx_baseline = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract local features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/datasets/Habitat/Hierarchical_Localization_outputs/sfm')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /datasets/Habitat/Hierarchical_Localization_outputs/sfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_features.main(feature_conf, images_path, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching pairs in database with SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_features.main(matcher_conf, sfm_pairs, feature_conf['output'], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangulate a new SfM model from the given poses\n",
    "We triangulate the sparse 3D pointcloud given the matches and the reference poses. These are obtained from the NVM SIFT model by generating a new COLMAP model without 3D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap_for_habitat.main(\n",
    "    dataset / '1LXtFkjw3qL_point0/images',\n",
    "    dataset / 'COLMAP/data.txt',\n",
    "    outputs / 'sfm_empty',\n",
    "    skip_points=True)\n",
    "\n",
    "triangulation.main(\n",
    "    reference_sfm,\n",
    "    outputs / 'sfm_empty',\n",
    "    images,\n",
    "    sfm_pairs,\n",
    "    outputs / f\"{feature_conf['output']}.h5\",\n",
    "    outputs / f\"{feature_conf['output']}_{matcher_conf['output']}_{sfm_pairs.stem}.h5\",\n",
    "    colmap_path='colmap')  # change if COLMAP is not in your PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the query images\n",
    "Here we assume that the localization pairs are already computed using image retrieval (NetVLAD). To generate new pairs from your own global descriptors, have a look at `hloc/pairs_from_retrieval.py`. These pairs are also used for the localization - see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_features.main(matcher_conf, loc_pairs, feature_conf['output'], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localize!\n",
    "Perform hierarchical localization using the precomputed retrieval and matches. The file `Aachen_hloc_superpoint+superglue_netvlad50.txt` will contain the estimated query poses. Have a look at `Aachen_hloc_superpoint+superglue_netvlad50.txt_logs.pkl` to analyze some statistics and find failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "localize_sfm.main(\n",
    "    reference_sfm / 'model',\n",
    "    dataset / 'query_with_intrinsics.txt',\n",
    "    loc_pairs,\n",
    "    outputs / f\"{feature_conf['output']}.h5\",\n",
    "    outputs / f\"{feature_conf['output']}_{matcher_conf['output']}_{loc_pairs.stem}.h5\",\n",
    "    results,\n",
    "    covisibility_clustering=False)  # not required with SuperPoint+SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the localization\n",
    "We parse the localization logs and for each query image plot matches and inliers with a few database images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualization.visualize_loc(\n",
    "    results, images, reference_sfm / 'model', n=100, top_k_db=1, prefix='', seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sfm_pairs.open()\n",
    "lines = f.readlines()\n",
    "filename1 = images / Path(lines[0].split(' ')[0])\n",
    "filename2 = images / Path(lines[1].split(' ')[1].rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of matched keypoints without RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sfm_pairs.open()\n",
    "lines = f.readlines()\n",
    "filename1 = images / Path(lines[0].split(' ')[0])\n",
    "filename2 = images / Path(lines[1].split(' ')[1].rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    line = lines[i]\n",
    "    print(line)\n",
    "    filename1 = images / Path(line.split(' ')[0])\n",
    "    filename2 = images / Path(line.split(' ')[1].rstrip('\\n'))\n",
    "    visualization.visualize_matches(outputs / feature_file, \n",
    "                                outputs / match_file, \n",
    "                                images, \n",
    "                                filename1, filename2, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization using SfM model of the map\n",
    "We visualize some of the registered images, and color their keypoint by visibility, track length, or triangulated depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_sfm_2d(sfm_dir / 'models/0', images, color_by='visibility', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_sfm_2d(sfm_dir / 'models/0', images, color_by='track_length', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_sfm_2d(sfm_dir / 'models/0', images, color_by='depth', n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing optimizationon selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx, cy = 128, 128\n",
    "fx, fy = 128, 128\n",
    "width, height = 256., 256.\n",
    "ransac_thresh = 12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_rotation_matrix(qvec):\n",
    "    r = R.from_quat([qvec[1], qvec[2], qvec[3], qvec[0]])\n",
    "    result = r.as_matrix()\n",
    "    result[:3,2] = -result[:3,2]\n",
    "    result[:3,1] = -result[:3,1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_3d(x, y, depth, fx, fy, cx, cy, cam_center_world, R_world_to_cam, w_in_quat_first = True):\n",
    "    if depth <= 0:\n",
    "        return 0\n",
    "    new_x = (x - cx)*depth/fx\n",
    "    new_y = (y - cy)*depth/fy\n",
    "    new_z = depth\n",
    "    coord_3D_world_to_cam = np.array([new_x, new_y, new_z], float)\n",
    "    if len(R_world_to_cam) == 4:\n",
    "        if w_in_quat_first:\n",
    "            matrix = quaternion_to_rotation_matrix(R_world_to_cam)\n",
    "        else:\n",
    "            R_world_to_cam = [R_world_to_cam[3], R_world_to_cam[0], R_world_to_cam[1], R_world_to_cam[2]]\n",
    "            matrix = quaternion_to_rotation_matrix(R_world_to_cam)\n",
    "    elif len(R_world_to_cam) == 3:\n",
    "        matrix = R_world_to_cam\n",
    "    coord_3D_cam_to_world = np.matmul(matrix, coord_3D_world_to_cam) + cam_center_world\n",
    "    return coord_3D_cam_to_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_to_pair(name0, name1):\n",
    "    return '_'.join((name0.replace('/', '-'), name1.replace('/', '-')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yaw_pitch_roll_from_quaternion(quaternion, w_first=False):\n",
    "    if not(w_first):\n",
    "        r = R.from_quat(quaternion)\n",
    "    else:\n",
    "        r = R.from_quat([quaternion[1], quaternion[2], quaternion[3] ,quaternion[0]])\n",
    "    return r.as_euler('yxz', degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yaw_pitch_roll_from_matrix(matrix):\n",
    "    r = R.from_matrix(matrix)\n",
    "    return(r.as_euler('yxz', degrees=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = '1LXtFkjw3qL_point0_query_0000'\n",
    "db_image = '1LXtFkjw3qL_point0_database_0004'\n",
    "path_to_hdf5_datasets = '/datasets/Habitat/1LXtFkjw3qL_point0/hdf5/'\n",
    "hdf5_filename_query = '_'.join(query_image.split('_')[:2]) + '.hdf5'\n",
    "hdf5_file_query = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query), 'r')\n",
    "hdf5_filename_db = '_'.join(db_image.split('_')[:2]) + '.hdf5'\n",
    "hdf5_file_db = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_db), 'r')\n",
    "num_query = int(query_image.split('_')[-1])\n",
    "num_db = int(db_image.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1LXtFkjw3qL_point0.hdf5'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_filename_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_file = h5py.File(str(outputs / feature_filename), 'r')\n",
    "match_filename = 'feats-superpoint-n4096-r1024-nms4_matches-superglue_NetVLAD_query_top_40.h5'\n",
    "match_file = h5py.File(str(outputs / match_filename), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_query = keypoints_file[query_image.rstrip('.png')]['keypoints'].__array__()\n",
    "keypoints_db = keypoints_file[db_image.rstrip('.png')]['keypoints'].__array__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11889225  0.2980008  -7.8185797 ]\n",
      "[-0.09069052  0.          0.99587911  0.        ]\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "images = {}\n",
    "images['query'] = {}\n",
    "images['db'] = {}\n",
    "images['query']['points_3d'] = []\n",
    "images['db']['points_3d'] = []\n",
    "points_3D = {}\n",
    "point_3D_id = 0\n",
    "image_id = 1\n",
    "depth_db = 10*hdf5_file_db['depth_base'][num_db]\n",
    "cam_xyz_db = hdf5_file_db['gps_base'][num_db]\n",
    "R_cam_db = hdf5_file_db['quat_base'][num_db]\n",
    "# R_cam_db = [1,0,0,0]\n",
    "# cam_xyz_db = [0,0,0]\n",
    "for keypoint in keypoints_db:\n",
    "    depth_keypoint = depth_db[int(keypoint[1]), int(keypoint[0])][0]\n",
    "    point_3d_xyz = get_point_3d(keypoint[0], keypoint[1], depth_keypoint, fx, fy, cx, cy, cam_xyz_db, R_cam_db)\n",
    "    if len(point_3d_xyz) != 1:\n",
    "        if (math.isnan(depth_keypoint)) or (math.isinf(depth_keypoint)):\n",
    "            images['db']['points_3d'].append(-1)\n",
    "            continue\n",
    "        else:\n",
    "            images['db']['points_3d'].append(point_3D_id)\n",
    "            points_3D[point_3D_id] = {}\n",
    "            points_3D[point_3D_id]['xyz'] = point_3d_xyz\n",
    "            if not('image_ids' in points_3D[point_3D_id].keys()):\n",
    "                points_3D[point_3D_id]['image_ids'] = []\n",
    "            if points_3D[point_3D_id]['image_ids'].count(image_id) == 0:\n",
    "                points_3D[point_3D_id]['image_ids'] = [image_id]\n",
    "            else:\n",
    "                points_3D[point_3D_id]['image_ids'].append(image_id)\n",
    "            point_3D_id += 1\n",
    "    else:\n",
    "        images['db']['points_3d'].append(-1)\n",
    "\n",
    "image_id = 2\n",
    "depth_query = 10*hdf5_file_query['depth'][num_query]\n",
    "cam_xyz = hdf5_file_query['gps'][num_query]\n",
    "R_cam = hdf5_file_query['quat'][num_query]\n",
    "print(cam_xyz)\n",
    "print(R_cam)\n",
    "for keypoint in keypoints_query:\n",
    "    depth_keypoint = depth_query[int(keypoint[1]), int(keypoint[0])][0]\n",
    "    point_3d_xyz = get_point_3d(keypoint[0], keypoint[1], depth_keypoint, fx, fy, cx, cy, cam_xyz, R_cam)\n",
    "    if len(point_3d_xyz) != 1:\n",
    "        if (math.isnan(depth_keypoint)) or (math.isinf(depth_keypoint)):\n",
    "            images['query']['points_3d'].append(-1)\n",
    "        else:\n",
    "            images['query']['points_3d'].append(point_3D_id)\n",
    "        points_3D[point_3D_id] = {}\n",
    "        points_3D[point_3D_id]['xyz'] = point_3d_xyz\n",
    "        if not('image_ids' in points_3D[point_3D_id].keys()):\n",
    "            points_3D[point_3D_id]['image_ids'] = []\n",
    "        if points_3D[point_3D_id]['image_ids'].count(image_id) == 0:\n",
    "            points_3D[point_3D_id]['image_ids'] = [image_id]\n",
    "        else:\n",
    "            points_3D[point_3D_id]['image_ids'].append(image_id)\n",
    "        point_3D_id += 1\n",
    "    else:\n",
    "        images['query']['points_3d'].append(-1)\n",
    "print(len(keypoints_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BundleAdjustment(g2o.SparseOptimizer):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Higher confident (better than CHOLMOD, according to \n",
    "        # paper \"3-D Mapping With an RGB-D Camera\")\n",
    "        solver = g2o.BlockSolverSE3(g2o.LinearSolverCSparseSE3())\n",
    "        solver = g2o.OptimizationAlgorithmLevenberg(solver)\n",
    "        super().set_algorithm(solver)\n",
    "        super().set_verbose(True)\n",
    "\n",
    "        # Convergence Criterion\n",
    "        terminate = g2o.SparseOptimizerTerminateAction()\n",
    "        terminate.set_gain_threshold(1e-6)\n",
    "        super().add_post_iteration_action(terminate)\n",
    "\n",
    "        # Robust cost Function (Huber function) delta\n",
    "        self.delta = np.sqrt(5.991)   \n",
    "        self.aborted = False\n",
    "\n",
    "    def optimize(self, max_iterations=10):\n",
    "        super().initialize_optimization()\n",
    "        super().optimize(max_iterations)\n",
    "        try:\n",
    "            return not self.aborted\n",
    "        finally:\n",
    "            self.aborted = False\n",
    "\n",
    "    def add_pose(self, pose_id, pose, cam, fixed=False):\n",
    "        sbacam = g2o.SBACam(\n",
    "            pose.orientation(), pose.position())\n",
    "        sbacam.set_cam(\n",
    "            cam.fx, cam.fy, cam.cx, cam.cy, cam.baseline)\n",
    "\n",
    "        v_se3 = g2o.VertexCam()\n",
    "        v_se3.set_id(pose_id * 2)\n",
    "        v_se3.set_estimate(sbacam)\n",
    "        v_se3.set_fixed(fixed)\n",
    "        super().add_vertex(v_se3) \n",
    "\n",
    "    def add_point(self, point_id, point, fixed=False, marginalized=True):\n",
    "        v_p = g2o.VertexSBAPointXYZ()\n",
    "        v_p.set_id(point_id * 2 + 1)\n",
    "        v_p.set_marginalized(marginalized)\n",
    "        v_p.set_estimate(point)\n",
    "        v_p.set_fixed(fixed)\n",
    "        super().add_vertex(v_p)\n",
    "\n",
    "    def add_edge(self, id, point_id, pose_id, meas):\n",
    "        if meas.is_stereo():\n",
    "            edge = self.stereo_edge(meas.xyx)\n",
    "        elif meas.is_left():\n",
    "            edge = self.mono_edge(meas.xy)\n",
    "        elif meas.is_right():\n",
    "            edge = self.mono_edge_right(meas.xy)\n",
    "\n",
    "        edge.set_id(id)\n",
    "        edge.set_vertex(0, self.vertex(point_id * 2 + 1))\n",
    "        edge.set_vertex(1, self.vertex(pose_id * 2))\n",
    "        kernel = g2o.RobustKernelHuber(self.delta)\n",
    "        edge.set_robust_kernel(kernel)\n",
    "        super().add_edge(edge)\n",
    "\n",
    "    def stereo_edge(self, projection, information=np.identity(3)):\n",
    "        e = g2o.EdgeProjectP2SC()\n",
    "        e.set_measurement(projection)\n",
    "        e.set_information(information)\n",
    "        return e\n",
    "\n",
    "    def mono_edge(self, projection, \n",
    "            information=np.identity(2) * 0.5):\n",
    "        e = g2o.EdgeProjectP2MC()\n",
    "        e.set_measurement(projection)\n",
    "        e.set_information(information)\n",
    "        return e\n",
    "\n",
    "    def mono_edge_right(self, projection, \n",
    "            information=np.identity(2) * 0.5):\n",
    "        e = g2o.EdgeProjectP2MCRight()\n",
    "        e.set_measurement(projection)\n",
    "        e.set_information(information)\n",
    "        return e\n",
    "\n",
    "    def get_pose(self, id):\n",
    "        return self.vertex(id * 2).estimate()\n",
    "\n",
    "    def get_point(self, id):\n",
    "        return self.vertex(id * 2 + 1).estimate()\n",
    "\n",
    "    def abort(self):\n",
    "        self.aborted = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera(object):\n",
    "    def __init__(self, fx, fy, cx, cy, width, height, baseline):\n",
    "        self.fx = fx\n",
    "        self.fy = fy\n",
    "        self.cx = cx\n",
    "        self.cy = cy\n",
    "        self.baseline = baseline\n",
    "\n",
    "        self.intrinsic = np.array([\n",
    "            [fx, 0, cx], \n",
    "            [0, fy, cy], \n",
    "            [0, 0, 1]])\n",
    "\n",
    "#         self.frustum_near = frustum_near\n",
    "#         self.frustum_far = frustum_far\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def compute_right_camera_pose(self, pose):\n",
    "        pos = pose * np.array([self.baseline, 0, 0])\n",
    "        return g2o.Isometry3d(pose.orientation(), pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMeasurement(object):\n",
    "    def __init__(self):\n",
    "        self.keyframe = None\n",
    "        self.mappoint = None\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return (self.keyframe.id, self.mappoint.id)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "    def __eq__(self, rhs):\n",
    "        return (isinstance(rhs, GraphMeasurement) and\n",
    "            self.id == rhs.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Measurement(GraphMeasurement):\n",
    "    \n",
    "    Source = Enum('Measurement.Source', ['TRIANGULATION', 'TRACKING', 'REFIND'])\n",
    "    Type = Enum('Measurement.Type', ['STEREO', 'LEFT', 'RIGHT'])\n",
    "\n",
    "    def __init__(self, type, source, keypoints):\n",
    "        super().__init__()\n",
    "\n",
    "        self.type = type\n",
    "        self.source = source\n",
    "        self.keypoints = keypoints\n",
    "        self.view = None    # mappoint's position in current coordinates frame\n",
    "\n",
    "        self.xy = np.array(self.keypoints[0].pt)\n",
    "        if self.is_stereo():\n",
    "            self.xyx = np.array([\n",
    "                *keypoints[0].pt, keypoints[1].pt[0]])\n",
    "\n",
    "        self.triangulation = (source == self.Source.TRIANGULATION)\n",
    "\n",
    "    def get_keypoint(self, i=0):\n",
    "        return self.keypoints[i]\n",
    "\n",
    "    def get_keypoints(self):\n",
    "        return self.keypoints\n",
    "\n",
    "    def is_stereo(self):\n",
    "        return self.type == Measurement.Type.STEREO\n",
    "    def is_left(self):\n",
    "        return self.type == Measurement.Type.LEFT\n",
    "    def is_right(self):\n",
    "        return self.type == Measurement.Type.RIGHT\n",
    "\n",
    "    def from_triangulation(self):\n",
    "        return self.triangulation\n",
    "    def from_tracking(self):\n",
    "        return self.source == Measurement.Source.TRACKING\n",
    "    def from_refind(self):\n",
    "        return self.source == Measurement.Source.REFIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMapPoint(object):\n",
    "    def __init__(self):\n",
    "        self.id = None\n",
    "        self.meas = dict()\n",
    "        self._lock = Lock()\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.id\n",
    "\n",
    "    def __eq__(self, rhs):\n",
    "        return (isinstance(rhs, GraphMapPoint) and \n",
    "            self.id == rhs.id)\n",
    "    def __lt__(self, rhs):\n",
    "        return self.id < rhs.id\n",
    "    def __le__(self, rhs):\n",
    "        return self.id <= rhs.id\n",
    "\n",
    "    def measurements(self):\n",
    "        with self._lock:\n",
    "            return self.meas.keys()\n",
    "\n",
    "    def keyframes(self):\n",
    "        with self._lock:\n",
    "            return self.meas.values()\n",
    "\n",
    "    def add_measurement(self, m):\n",
    "        with self._lock:\n",
    "            self.meas[m] = m.keyframe\n",
    "\n",
    "    def remove_measurement(self, m):\n",
    "        with self._lock:\n",
    "            try:\n",
    "                del self.meas[m]\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapPoint(GraphMapPoint):\n",
    "    _id = 0\n",
    "    _id_lock = Lock()\n",
    "\n",
    "    def __init__(self, position, covariance=np.identity(3) * 1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        with MapPoint._id_lock:\n",
    "            self.id = MapPoint._id\n",
    "            MapPoint._id += 1\n",
    "\n",
    "        self.position = position\n",
    "        self.covariance = covariance\n",
    "        # self.owner = None\n",
    "\n",
    "        self.count = defaultdict(int)\n",
    "\n",
    "    def update_position(self, position):\n",
    "        self.position = position\n",
    "    def update_normal(self, normal):\n",
    "        self.normal = normal\n",
    "    def update_descriptor(self, descriptor):\n",
    "        self.descriptor = descriptor\n",
    "    def set_color(self, color):\n",
    "        self.color = color\n",
    "\n",
    "    def is_bad(self):\n",
    "        with self._lock:\n",
    "            status =  (\n",
    "                self.count['meas'] == 0\n",
    "                or (self.count['outlier'] > 20\n",
    "                    and self.count['outlier'] > self.count['inlier'])\n",
    "                or (self.count['proj'] > 20\n",
    "                    and self.count['proj'] > self.count['meas'] * 10))\n",
    "            return status\n",
    "\n",
    "    def increase_outlier_count(self):\n",
    "        with self._lock:\n",
    "            self.count['outlier'] += 1\n",
    "    def increase_inlier_count(self):\n",
    "        with self._lock:\n",
    "            self.count['inlier'] += 1\n",
    "    def increase_projection_count(self):\n",
    "        with self._lock:\n",
    "            self.count['proj'] += 1\n",
    "    def increase_measurement_count(self):\n",
    "        with self._lock:\n",
    "            self.count['meas'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx, fy = 128., 128.\n",
    "cx, cy = 128., 128.\n",
    "width, height = 256, 256\n",
    "fx_baseline = 300\n",
    "baseline = fx_baseline/fx\n",
    "cam = Camera(\n",
    "    fx, fy, cx, cy, \n",
    "    width, height, \n",
    "    baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = names_to_pair(query_image, db_image)\n",
    "matches = match_file[pair]['matches0'].__array__()\n",
    "valid = np.where(matches > -1)[0]\n",
    "# valid - массив точек на query изображении, которые имеют сопоставление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3578"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1LXtFkjw3qL_point0_query_0000\n",
      "1LXtFkjw3qL_point0_database_0004\n"
     ]
    }
   ],
   "source": [
    "print(query_image)\n",
    "print(db_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For Query image\n",
    "\n",
    "data = {\n",
    "    'points_db'   : [],\n",
    "    'points_g2o'  : [],\n",
    "    'points_query': []\n",
    "}\n",
    "\n",
    "optimizer = BundleAdjustment()\n",
    "optimizer.clear()\n",
    "pose_44 = np.eye(4)\n",
    "R_cam = quaternion_to_rotation_matrix(hdf5_file_db['quat_base'][num_db])\n",
    "\n",
    "r = R.from_quat([hdf5_file_db['quat_base'][num_db][1], hdf5_file_db['quat_base'][num_db][2], hdf5_file_db['quat_base'][num_db][3], hdf5_file_db['quat_base'][num_db][0]])\n",
    "matrix = r.as_matrix()\n",
    "\n",
    "pose_44[:3,:3] = quaternion_to_rotation_matrix(hdf5_file_db['quat_base'][num_db])\n",
    "pose_44[:3,3] = hdf5_file_db['gps_base'][num_db]\n",
    "pose = g2o.Isometry3d(pose_44)\n",
    "optimizer.add_pose(0, pose, cam, fixed=False)\n",
    "measurements = []\n",
    "for i, keypoint_xy_query in enumerate(keypoints_query):\n",
    "    # Если кипоинт не имеет сопоставления, то пропускаем его\n",
    "    if list(valid).count(i) == 0:\n",
    "        continue\n",
    "    # Получим индекс соответствующего кипоинта на database изображении\n",
    "    kp_idx_db = matches[i]\n",
    "    # Получим индекс point 3D, соответствующий индексу ключевой точки kp_idx_db\n",
    "    point_3D_id_db = images['db']['points_3d'][kp_idx_db]\n",
    "    point_3D_id_query = images['query']['points_3d'][i]\n",
    "    \n",
    "    point_xyz_db    = points_3D[point_3D_id_db]['xyz']\n",
    "    point_xyz_query = points_3D[point_3D_id_query]['xyz']\n",
    "    \n",
    "    data['points_db'].append(point_xyz_db.tolist())\n",
    "    data['points_query'].append(point_xyz_query.tolist())\n",
    "    \n",
    "    x_right = keypoint_xy_query[0] - fx_baseline/(depth_query[int(keypoint_xy_query[1]), int(keypoint_xy_query[0])][0])\n",
    "    kp_left, kp_right = cv2.KeyPoint(), cv2.KeyPoint()\n",
    "    kp_left.pt  = (int(keypoint_xy_query[0]), int(keypoint_xy_query[1]))\n",
    "    kp_right.pt = (int(x_right), int(keypoint_xy_query[1]))\n",
    "    meas = Measurement(Measurement.Type.STEREO,\n",
    "                       Measurement.Source.TRIANGULATION,\n",
    "                       [kp_left, kp_right])\n",
    "    mappoint = MapPoint(point_xyz_db)\n",
    "    meas.mappoint = mappoint\n",
    "    measurements.append(meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For database image\n",
    "# optimizer = BundleAdjustment()\n",
    "# optimizer.clear()\n",
    "# pose_44 = np.eye(4)\n",
    "# R_cam = hdf5_file_db['quat_base'][num_db]\n",
    "# r = R.from_quat([R_cam[1], R_cam[2], R_cam[3], R_cam[0]])\n",
    "# pose_44[:3,:3] = r.as_matrix()\n",
    "# pose_44[:3,3] = hdf5_file_db['gps_base'][num_db]\n",
    "# pose = g2o.Isometry3d(pose_44)\n",
    "# optimizer.add_pose(0, pose, cam, fixed=False)\n",
    "# measurements = []\n",
    "# for i, keypoint in enumerate(keypoints_db):\n",
    "#     kp_idx_db = i\n",
    "#     # Получим индекс point 3D, соответствующий индексу ключевой точки kp_idx_db\n",
    "#     point_3D_id = images['db']['points_3d'][i]\n",
    "#     point = points_3D[point_3D_id]['xyz']\n",
    "    \n",
    "#     x_right = keypoint[0] - fx_baseline/(depth_db[int(keypoint[1]), int(keypoint[0])][0])\n",
    "#     kp_left, kp_right = cv2.KeyPoint(), cv2.KeyPoint()\n",
    "#     kp_left.pt  = (keypoint[0], keypoint[1])\n",
    "#     kp_right.pt = (x_right, keypoint[1])\n",
    "#     meas = Measurement(Measurement.Type.STEREO,\n",
    "#                        Measurement.Source.TRIANGULATION,\n",
    "#                        [kp_left, kp_right])\n",
    "#     mappoint = MapPoint(point)\n",
    "#     meas.mappoint = mappoint\n",
    "#     measurements.append(meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(measurements):\n",
    "    optimizer.add_point(i, m.mappoint.position, fixed=True)\n",
    "    optimizer.add_edge(0, i, 0, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g2o result:\n",
      "[ 0.13260147  0.304025   -7.92440136]\n",
      "[-1.06120887e+01 -8.48145196e-02 -1.79837403e+02]\n",
      "\n",
      "query gt:\n",
      "[ 0.11889225  0.2980008  -7.8185797 ]\n",
      "[-10.40666736   0.         180.        ]\n",
      "\n",
      "db pose:\n",
      "[-0.10379431  0.26535606 -7.9203405 ]\n",
      "[-21.10587093   0.         180.        ]\n",
      "\n",
      "distance error - 0.10687592142804575\n",
      "angle error - 0.27546089226920856\n",
      "pose:\n",
      "[[-9.82891784e-01  2.83785101e-03  1.84162121e-01  1.32601467e-01]\n",
      " [-3.06192455e-03 -9.99994878e-01 -9.32352448e-04  3.04025000e-01]\n",
      " [ 1.84158532e-01 -1.48029208e-03  9.82895439e-01 -7.92440136e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "optimizer.optimize(10)\n",
    "result = optimizer.get_pose(0)\n",
    "pose_estimated = result.matrix()\n",
    "\n",
    "r = R.from_matrix(result.matrix()[:3,:3])\n",
    "yaw_pitch_roll = r.as_euler('yxz', degrees=True)\n",
    "\n",
    "print('g2o result:')\n",
    "print(result.position())\n",
    "print(yaw_pitch_roll)\n",
    "print()\n",
    "cam_xyz = hdf5_file_query['gps'][num_query]\n",
    "pose_query = np.eye(4)\n",
    "pose_query[:3,3] = cam_xyz\n",
    "pose_query[:3,:3] = quaternion_to_rotation_matrix(hdf5_file_query['quat'][num_query])\n",
    "print('query gt:')\n",
    "print(cam_xyz)\n",
    "r = R.from_matrix(pose_query[:3,:3])\n",
    "yaw_pitch_roll = r.as_euler('yxz', degrees=True)\n",
    "print(yaw_pitch_roll)\n",
    "print()\n",
    "cam_xyz_db = hdf5_file_db['gps_base'][num_db]\n",
    "r = R.from_matrix(quaternion_to_rotation_matrix(hdf5_file_query['quat_base'][num_db]))\n",
    "yaw_pitch_roll = r.as_euler('yxz', degrees=True)\n",
    "\n",
    "# pose_estimated = np.eye(4)\n",
    "# pose_estimated[:3,3] = cam_xyz_db\n",
    "# pose_estimated[:3,:3] = r.as_matrix()\n",
    "\n",
    "print('db pose:')\n",
    "print(cam_xyz_db)\n",
    "print(yaw_pitch_roll)\n",
    "\n",
    "error_pose = np.linalg.inv(pose_estimated) @ pose_query\n",
    "dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "r = R.from_matrix(error_pose[:3, :3])\n",
    "rotvec = r.as_rotvec()\n",
    "angle_error = (np.sum(rotvec**2)**0.5) * 180 / 3.14159265353\n",
    "angle_error = abs(90 - abs(angle_error-90))\n",
    "print()\n",
    "print('distance error - {}'.format(dist_error))\n",
    "print('angle error - {}'.format(angle_error))\n",
    "print('pose:')\n",
    "print(result.matrix())\n",
    "data['cam_pose_g2o'] = result.matrix().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keypoints_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, keypoint_xy_query in enumerate(keypoints_query):\n",
    "    # Если кипоинт не имеет сопоставления, то пропускаем его\n",
    "    if list(valid).count(i) == 0:\n",
    "        continue\n",
    "    depth_keypoint = depth_query[int(keypoint_xy_query[1]), int(keypoint_xy_query[0])][0]\n",
    "    point_3d_xyz_g2o = get_point_3d(keypoint_xy_query[0], keypoint_xy_query[1], depth_keypoint, fx, fy, cx, cy, result.matrix()[:3,3], result.matrix()[:3,:3])\n",
    "    point_3d_xyz_query = get_point_3d(keypoint_xy_query[0], keypoint_xy_query[1], depth_keypoint, fx, fy, cx, cy, pose_query[:3,3], pose_query[:3,:3])\n",
    "    if len(point_3d_xyz) != 1:\n",
    "        if (math.isnan(depth_keypoint)) or (math.isinf(depth_keypoint)):\n",
    "            continue\n",
    "        else:\n",
    "#             data['points_query'].append(point_3d_xyz_query.tolist())\n",
    "            data['points_g2o'].append(point_3d_xyz_g2o.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/g2o_data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keypoints_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair = names_to_pair(query_image, db_image)\n",
    "# if not(pair in match_file.keys()):\n",
    "#     pair = names_to_pair(db_image, query_image)\n",
    "# matches - список индексов ключевых точек на query изображении в порядке сопоставления с кипоинтами db\n",
    "# matches = match_file[pair]['matches0'].__array__()\n",
    "matches = np.arange(len(keypoints_db))\n",
    "# valid - список индексов ключевых точек на query изображении, которые имеют сопоставление\n",
    "valid = np.where(matches > -1)[0]\n",
    "# matched_keypoints_query, matched_keypoints_db = keypoints_query[valid], keypoints_db[matches[valid]]\n",
    "# num_matches = len(matched_keypoints_query)\n",
    "points3D_ids = np.array(images['db']['points_3d'])\n",
    "# matched_keypoints_query = keypoints_db\n",
    "valid = valid[points3D_ids[matches[valid]] != -1]\n",
    "kp_idx_to_3D = defaultdict(list)\n",
    "kp_idx_to_3D_to_db = defaultdict(lambda: defaultdict(list))\n",
    "i = 0\n",
    "# Пробегаемся по индексам ключевых точек на изображении query, которые имеют сопоставление с db\n",
    "# for idx in valid:\n",
    "#     # берем индекс (idx) ключевой точки на изображении query;\n",
    "#     # затем берем индекс кипоинта на изображении db, соответствующий этому кипоинту на query -  matches[idx]\n",
    "#     # затем получаем индекс 3D ключевой точки по ключу индекса ключевой точки на db изображении\n",
    "#     id_3D = points3D_ids[matches[idx]]\n",
    "#     kp_idx_to_3D_to_db[idx][id_3D].append(i)\n",
    "#     # avoid duplicate observations\n",
    "#     if id_3D not in kp_idx_to_3D[idx]:\n",
    "#         kp_idx_to_3D[idx].append(id_3D)\n",
    "idxs = list(kp_idx_to_3D.keys())\n",
    "mkp_idxs = [i for i in idxs for _ in kp_idx_to_3D[i]]\n",
    "mkpq = keypoints_query[mkp_idxs]\n",
    "# mkpq += 0.5  # COLMAP coordinates\n",
    "\n",
    "mp3d_ids = [j for i in idxs for j in kp_idx_to_3D[i]]\n",
    "\n",
    "mp3d = [list(points_3D[num_keypoint_in_db]['xyz']) for num_keypoint_in_db in valid if \\\n",
    "             images['db']['points_3d'][num_keypoint_in_db] != -1]\n",
    "# mp3d = [points_3D[j]['xyz'] for j in mp3d_ids]\n",
    "# mp3d = np.array(mp3d).reshape(-1, 3)\n",
    "\n",
    "# mostly for logging and post-processing\n",
    "# mkp_to_3D_to_db = [(j, kp_idx_to_3D_to_db[i][j])\n",
    "#                    for i in idxs for j in kp_idx_to_3D[i]]\n",
    "\n",
    "# camera_model, width, height, params = qinfo\n",
    "cfg = {\n",
    "       'model': 'PINHOLE',\n",
    "       'width': int(width),\n",
    "       'height': int(height),\n",
    "       'params': np.array([fx, fy, cx, cy]),\n",
    "}\n",
    "# matched_keypoints_query += 0.5\n",
    "keypoints_db += 0.5\n",
    "ret = pycolmap.absolute_pose_estimation(keypoints_db, mp3d, cfg, 24)\n",
    "print(ret['success'])\n",
    "print(ret['tvec'])\n",
    "print(yaw_pitch_roll_from_quaternion(list(ret['qvec']), w_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret['qvec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 6 images of base point. HFOV=90 degrees. TURN_ANGLE=60 degrees\n",
    "num_db = 4\n",
    "rgb_base = hdf5_file_db['rgb_base'][num_db]\n",
    "rgb = hdf5_file_query['rgb'][num_query]\n",
    "gps_base = hdf5_file_db['gps_base']\n",
    "gps = hdf5_file_query['gps']\n",
    "f = plt.figure(figsize=(10,10))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax1.set_title('query')\n",
    "ax2 = f.add_subplot(122)\n",
    "ax2.imshow(rgb_base)\n",
    "ax1.imshow(rgb)\n",
    "ax2.set_title('database')\n",
    "\n",
    "db_quat_wxyz = hdf5_file_db['quat_base'][num_db]\n",
    "r = R.from_quat([db_quat_wxyz[1], db_quat_wxyz[2], db_quat_wxyz[3], db_quat_wxyz[0]])\n",
    "euler_x_db, euler_y_db, euler_z_db = r.as_euler('xyz', degrees=True)\n",
    "print('database pose:')\n",
    "print('    translation: {}'.format(gps_base[num_db]))\n",
    "print('    yaw, pitch, roll: {:.2f}, {:.2f}, {:.2f}'.format(euler_x_db, euler_y_db, euler_z_db))\n",
    "\n",
    "r = R.from_quat([ret['qvec'][1], ret['qvec'][2], ret['qvec'][3], ret['qvec'][0]])\n",
    "euler_x_pred, euler_y_pred, euler_z_pred = r.as_euler('xyz', degrees=True)\n",
    "print()\n",
    "print('query pose calculated:')\n",
    "print('    translation: {}'.format(ret['tvec']))\n",
    "print('    yaw, pitch, roll: {:.2f}, {:.2f}, {:.2f}'.format(euler_x_pred, euler_y_pred, euler_z_pred))\n",
    "\n",
    "gt_quat_wxyz = hdf5_file_query['quat'][num_query]\n",
    "r = R.from_quat([gt_quat_wxyz[1], gt_quat_wxyz[2], gt_quat_wxyz[3], gt_quat_wxyz[0]])\n",
    "euler_x_gt, euler_y_gt, euler_z_gt = r.as_euler('xyz', degrees=True)\n",
    "print()\n",
    "print('query pose gt:')\n",
    "print('    translation: {}'.format(gps[num_query]))\n",
    "print('    yaw, pitch, roll: {:.2f}, {:.2f}, {:.2f}'.format(euler_x_gt, euler_y_gt, euler_z_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_images = images_path\n",
    "first_image = Path(db_image)\n",
    "second_image = Path(query_image)\n",
    "keypoints_filename = Path(feature_file)\n",
    "visualization.visualize_matches(str(outputs / keypoints_filename), str(outputs / match_filename), root_dir_images, os.path.join(root_dir_images, second_image), os.path.join(root_dir_images, first_image), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_new = []\n",
    "for i, keypoint in enumerate(keypoints_db):\n",
    "    kp_idx_db = i\n",
    "    # Получим индекс point 3D, соответствующий индексу ключевой точки kp_idx_db\n",
    "    point_3D_id = images['db']['points_3d'][i]\n",
    "    point = points_3D[point_3D_id]['xyz']\n",
    "    x_right = keypoint[0] - fx_baseline/(depth_db[int(keypoint[1]), int(keypoint[0])][0])\n",
    "    kp_left, kp_right = cv2.KeyPoint(), cv2.KeyPoint()\n",
    "    kp_left.pt  = (keypoint[0], keypoint[1])\n",
    "    kp_right.pt = (x_right, keypoint[1])\n",
    "    meas = Measurement(Measurement.Type.STEREO,\n",
    "                       Measurement.Source.TRIANGULATION,\n",
    "                       [kp_left, kp_right])\n",
    "    mappoint = MapPoint(point)\n",
    "    meas.mappoint = mappoint\n",
    "    measurements.append(meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose = result.matrix()\n",
    "rotation_matrix = cam_pose[:3,:3]\n",
    "rotation_matrix = quaternion_to_rotation_matrix(R_cam_db)\n",
    "# cam_pose = np.eye(4)\n",
    "# cam_pose[:3,:3] = rotation_matrix\n",
    "# cam_pose[:3,3] = cam_xyz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_kps = []\n",
    "for point_3D_id, point_3D in points_3D.items():\n",
    "    if point_3D['image_ids'] == [2]:\n",
    "        continue\n",
    "    xyz = point_3D['xyz'] - cam_pose[:3, 3]\n",
    "    xyz = np.matmul(np.linalg.inv(rotation_matrix), xyz)\n",
    "    x = fx * xyz[0] / xyz[2] + cx\n",
    "    y = fy * xyz[1] / xyz[2] + cy\n",
    "    list_of_kps.append([int(x),int(y)])\n",
    "array_of_kps = np.array([np.array(xi) for xi in list_of_kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.viz import plot_images, plot_keypoints, plot_matches, cm_RdGn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    appendix = str(path).split('_')\n",
    "    directory = str(path)[:str(path).rfind('/')]\n",
    "    subdir = appendix[-4][appendix[-4].rfind('/'):]+'_'+appendix[-3]\n",
    "    subdir = subdir[1:]\n",
    "    filename = str(path).split('/')[-1] + '.png'\n",
    "#     path = directory + '/' + subdir + '/' +  filename\n",
    "    path = directory + '/' +  filename\n",
    "    path = Path(path)\n",
    "    assert path.exists(), path\n",
    "    image = cv2.imread(str(path))\n",
    "    if len(image.shape) == 3:\n",
    "        image = image[:, :, ::-1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image(os.path.join(images_path, db_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([image], dpi=300)\n",
    "plot_keypoints([array_of_kps], colors='lime', ps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([image], dpi=300)\n",
    "plot_keypoints([keypoints_db], colors='lime', ps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting poses for query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_rotation_matrix(quaternion_wxyz):\n",
    "    r = R.from_quat([quaternion_wxyz[1], quaternion_wxyz[2], quaternion_wxyz[3], quaternion_wxyz[0]])\n",
    "    matrix = r.as_matrix()\n",
    "    matrix[:3,2] = -matrix[:3,2]\n",
    "    matrix[:3,1] = -matrix[:3,1]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_3d(x, y, depth, fx, fy, cx, cy, cam_center_world, R_world_to_cam, w_in_quat_first = True):\n",
    "    if depth <= 0:\n",
    "        return 0\n",
    "    new_x = (x - cx)*depth/fx\n",
    "    new_y = (y - cy)*depth/fy\n",
    "    new_z = depth\n",
    "    coord_3D_world_to_cam = np.array([new_x, new_y, new_z], float)\n",
    "    if len(R_world_to_cam) == 4:\n",
    "        if w_in_quat_first:\n",
    "            matrix = quaternion_to_rotation_matrix(R_world_to_cam)\n",
    "        else:\n",
    "            R_world_to_cam = [R_world_to_cam[3], R_world_to_cam[0], R_world_to_cam[1], R_world_to_cam[2]]\n",
    "            matrix = quaternion_to_rotation_matrix(R_world_to_cam)\n",
    "    coord_3D_cam_to_world = np.matmul(matrix, coord_3D_world_to_cam) + cam_center_world\n",
    "    return coord_3D_cam_to_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yaw_pitch_roll_from_quaternion(quaternion, w_first=False):\n",
    "    if not(w_first):\n",
    "        r = R.from_quat(quaternion)\n",
    "    else:\n",
    "        r = R.from_quat([quaternion[1], quaternion[2], quaternion[3] ,quaternion[0]])\n",
    "    return r.as_euler('yxz', degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_pairs = '/datasets/Habitat/NetVLAD.txt'\n",
    "retrievals = parse_retrieval(loc_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_hdf5_datasets = '/datasets/Habitat/1LXtFkjw3qL_point0/hdf5/'\n",
    "keypoints_file = h5py.File(str(outputs / feature_filename), 'r')\n",
    "match_filename = 'feats-superpoint-n4096-r1024-nms4_matches-superglue_NetVLAD_query_top_40.h5'\n",
    "match_file = h5py.File(str(outputs / match_filename), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx, fy = 128., 128.\n",
    "cx, cy = 128., 128.\n",
    "width, height = 256, 256\n",
    "fx_baseline = 300\n",
    "baseline = fx_baseline/fx\n",
    "\n",
    "cam = Camera(\n",
    "    fx, fy, cx, cy, \n",
    "    width, height, \n",
    "    baseline)\n",
    "\n",
    "top_num_db_image = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 950/950 [00:26<00:00, 35.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# G2O\n",
    "\n",
    "data = {\n",
    "    'query'    : {},\n",
    "    'database' : {},\n",
    "    'g2o'      : {}\n",
    "}\n",
    "\n",
    "optimizer = BundleAdjustment()\n",
    "results = {}\n",
    "metrics = {\n",
    "    \"(5m, 20°)\"   : 0,\n",
    "    \"(1m, 10°)\"   : 0,\n",
    "    \"(0.5m, 5°)\"  : 0,\n",
    "    \"(0.25m, 2°)\" : 0\n",
    "}\n",
    "max_angle_error = 0\n",
    "for query_image, db_images in tqdm(retrievals.items()):\n",
    "#     if query_image != '1LXtFkjw3qL_point5_query_0000':\n",
    "#         continue\n",
    "    measurements = []\n",
    "    optimizer.clear()\n",
    "    hdf5_filename_query = '_'.join(query_image.split('_')[:2]) + '.hdf5'\n",
    "    hdf5_file_query = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query), 'r')\n",
    "    \n",
    "    num_query = int(query_image.split('_')[-1])\n",
    "    \n",
    "    depth_query = 10*hdf5_file_query['depth'][num_query]\n",
    "    \n",
    "    # keypoints_query - массив из пар координта (x,y), которые являются кипоинтами на query image\n",
    "    keypoints_query = keypoints_file[query_image]['keypoints'].__array__()\n",
    "    \n",
    "    # all_matched_keypoints_ids - массив из true/false, говорящий о том, имеет ли совпадение кипоинт с соответствующим индексом\n",
    "    all_matched_keypoints_ids = np.full((len(keypoints_query)), False)\n",
    "    \n",
    "    for num in range(len(db_images)):\n",
    "        if num >= top_num_db_image:\n",
    "            break\n",
    "        db_image = db_images[num]\n",
    "#         if db_image != '1LXtFkjw3qL_point5_database_0004':\n",
    "#             continue\n",
    "        hdf5_filename_db = '_'.join(db_image.split('_')[:2]) + '.hdf5'\n",
    "        hdf5_file_db = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_db), 'r')\n",
    "        \n",
    "        num_db = int(db_image.split('_')[-1])\n",
    "        \n",
    "        depth_db = 10*hdf5_file_db['depth_base'][num_db]\n",
    "        \n",
    "        translation = hdf5_file_db['gps_base'][num_db]\n",
    "        orientation = quaternion_to_rotation_matrix(hdf5_file_db['quat_base'][num_db])\n",
    "        pose_44 = np.eye(4)\n",
    "        pose_44[:3,:3] = orientation\n",
    "        pose_44[:3,3] = translation\n",
    "        data['database']['cam_pose'] = pose_44\n",
    "        pose = g2o.Isometry3d(pose_44)\n",
    "        optimizer.add_pose(0, pose, cam, fixed=False)\n",
    "        \n",
    "        keypoints_db = keypoints_file[db_image]['keypoints'].__array__()\n",
    "        \n",
    "        # matches_db_keypoints_ids - массив из индексов кипоинтов на database image, которые совпадают с кипоинтом, индекс которого равен положению в массиве\n",
    "        matches_db_keypoints_ids = match_file['_'.join([query_image,db_image])]['matches0'].__array__()\n",
    "        \n",
    "        # matched_keypoints_query_ids - массив из индексов кипоинтов на query image, которые имеют сопоставление\n",
    "        matched_keypoints_query_ids = np.where(matches_db_keypoints_ids > -1)[0]\n",
    "        \n",
    "        # Пробегаемся по всем индексам кипоинтов query image, которые имеют сопоставление с database image\n",
    "        for keypoint_id_query in matched_keypoints_query_ids:\n",
    "            if all_matched_keypoints_ids[keypoint_id_query] == False:\n",
    "                \n",
    "                # затем получаем индекс сопоставленного кипоинта на database image\n",
    "                keypoint_id_database = matches_db_keypoints_ids[keypoint_id_query]\n",
    "                \n",
    "                # получаем координаты x,y для сопоставленных кипоинтов на query и database\n",
    "                keypoint_xy_query = keypoints_query[keypoint_id_query]\n",
    "                keypoint_xy_db = keypoints_db[keypoint_id_database]\n",
    "                \n",
    "                # получаем глубины данных кипоинтов\n",
    "                depth_keypoint_query = depth_query[int(keypoint_xy_query[1]), int(keypoint_xy_query[0])][0]\n",
    "                depth_keypoint_db = depth_db[int(keypoint_xy_db[1]), int(keypoint_xy_db[0])][0]\n",
    "                \n",
    "                # если глубина кипоинтов <=0, то она скорее всего невалидна, пропускаем ее\n",
    "                if depth_keypoint_db <= 0 or depth_keypoint_query <= 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Получаем координаты виртуального кипоинта для виртуальной правой камеры на query image\n",
    "                x_right = keypoint_xy_query[0] - fx_baseline/depth_keypoint_query\n",
    "                \n",
    "                # Создаем объекты кипоинтов (на query image, left and right cam), подходящие для погрузки их в g2o\n",
    "                kp_left, kp_right = cv2.KeyPoint(), cv2.KeyPoint()\n",
    "                kp_left.pt  = (keypoint_xy_query[0], keypoint_xy_query[1])\n",
    "                kp_right.pt = (x_right, keypoint_xy_query[1])\n",
    "                meas = Measurement(Measurement.Type.STEREO,\n",
    "                       Measurement.Source.TRIANGULATION,\n",
    "                       [kp_left, kp_right])\n",
    "                \n",
    "                #  Теперь нужно загрузить в g2o 3D координаты кипоинта на database image. Этот кипоинт сопоставлен\n",
    "                # с кипоинтом на query image\n",
    "                # Получаем 3D координату ключевой точки на database image\n",
    "                global_xyz_of_keypoint = get_point_3d(keypoint_xy_db[0], keypoint_xy_db[1], depth_keypoint_db, \\\n",
    "                                                      fx, fy, cx, cy, \\\n",
    "                                                      translation, hdf5_file_db['quat_base'][num_db])\n",
    "                mappoint = MapPoint(global_xyz_of_keypoint)\n",
    "                # загружаем 3D координату кипоинта в объект meas, который будет теперь содержать координаты (x,y)\n",
    "                # киопинта на query image и 3D координаты (x,y,z) кипоинта на database image \n",
    "                meas.mappoint = mappoint\n",
    "                measurements.append(meas)\n",
    "                \n",
    "                # Помечаем этот кипоинт на query image тем, что в дальнейшем его не нужно рассматривать - его уже\n",
    "                # добавили в измерения\n",
    "                all_matched_keypoints_ids[keypoint_id_query] = True\n",
    "    for i, m in enumerate(measurements):\n",
    "        optimizer.add_point(i, m.mappoint.position, fixed=True)\n",
    "        optimizer.add_edge(0, i, 0, m)\n",
    "    optimizer.optimize(10)\n",
    "    result = optimizer.get_pose(0)\n",
    "    data['g2o']['cam_pose'] = result\n",
    "    \n",
    "    pose_estimated = np.eye(4)\n",
    "    pose_query = np.eye(4)\n",
    "    \n",
    "#     r = R.from_quat([orientation_quat_wxyz[1], orientation_quat_wxyz[2], orientation_quat_wxyz[3], orientation_quat_wxyz[0]])\n",
    "    \n",
    "#     # Now get pose from g2o optimization \n",
    "    matrix = result.matrix()[:3,:3]\n",
    "    pose_estimated[:3, :3] = matrix\n",
    "    pose_estimated[:3, 3] = result.position()\n",
    "#     print(\"Estimated translation:\")\n",
    "#     print(result.position())\n",
    "#     print(matrix)\n",
    "\n",
    "    # Now get pose from NetVLAD\n",
    "    pose_estimated[:3, :3] = quaternion_to_rotation_matrix(hdf5_file_db['quat_base'][num_db])\n",
    "    pose_estimated[:3, 3] = hdf5_file_db['gps_base'][num_db]\n",
    "    \n",
    "#     orientation_quat_xyzw = [hdf5_file_query['quat'][num_query][1], hdf5_file_query['quat'][num_query][2],\\\n",
    "#                             hdf5_file_query['quat'][num_query][3], hdf5_file_query['quat'][num_query][0]]\n",
    "#     r = R.from_quat(orientation_quat_xyzw)\n",
    "#     matrix = r.as_matrix()\n",
    "    pose_query[:3, :3] = quaternion_to_rotation_matrix(hdf5_file_query['quat'][num_query])\n",
    "    pose_query[:3, 3] = hdf5_file_query['gps'][num_query]\n",
    "    data['query']['cam_pose'] = pose_query \n",
    "#     print(\"GT translation:\")\n",
    "#     print(hdf5_file_query['gps'][num_query])\n",
    "    error_pose = np.linalg.inv(pose_estimated) @ pose_query\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = R.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / 3.14159265353\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "#     if angle_error > max_angle_error:\n",
    "#         max_angle_error = angle_error\n",
    "#         print(query_image)\n",
    "#         print(db_image)\n",
    "#         print(max_angle_error)\n",
    "#         print(\"len of measurements = {}\".format(len(measurements)))\n",
    "#         print()\n",
    "#         print()\n",
    "#     print(\"angle error = {.f}\", angle_error)\n",
    "#     print(\"dist error = {.f}\", dist_error)\n",
    "#         print(db_image)\n",
    "#         print(query_image)\n",
    "#         print()\n",
    "\n",
    "#     results[query_image] = {\n",
    "#         'xyz': list(map(float, result.position())),\n",
    "#         'quaternion_xyzw': list(map(float,r.as_quat()))\n",
    "#     }\n",
    "#     if angle_error < 20 and dist_error < 5:\n",
    "#         metrics[\"(5m, 20°)\"] += 1\n",
    "#     if angle_error < 10 and dist_error < 1:\n",
    "#         metrics[\"(1m, 10°)\"] += 1\n",
    "#     if angle_error < 5 and dist_error < 0.5:\n",
    "#         metrics[\"(0.5m, 5°)\"] += 1\n",
    "#     if angle_error < 2 and dist_error < 0.25:\n",
    "#         metrics[\"(0.25m, 2°)\"] += 1\n",
    "    if dist_error < 5:\n",
    "        metrics[\"(5m, 20°)\"] += 1\n",
    "    if dist_error < 1:\n",
    "        metrics[\"(1m, 10°)\"] += 1\n",
    "    if dist_error < 0.5:\n",
    "        metrics[\"(0.5m, 5°)\"] += 1\n",
    "    if dist_error < 0.25:\n",
    "        metrics[\"(0.25m, 2°)\"] += 1\n",
    "#     break\n",
    "for key in metrics.keys():\n",
    "    metrics[key] = metrics[key] / 950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(5m, 20°)': 0.9705263157894737,\n",
       " '(1m, 10°)': 0.9189473684210526,\n",
       " '(0.5m, 5°)': 0.9189473684210526,\n",
       " '(0.25m, 2°)': 0.5021052631578947}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "с использованием магической строчки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RANSAC\n",
    "\n",
    "results = {}\n",
    "metrics = {\n",
    "    \"(5m, 20°)\": 0,\n",
    "    \"(1m, 10°)\": 0,\n",
    "    \"(0.5m, 5°)\": 0,\n",
    "    \"(0.25m, 2°)\": 0\n",
    "}\n",
    "for query_image, db_images in tqdm(retrievals.items()):\n",
    "    keypoints_2D = []\n",
    "    keypoints_3D = []\n",
    "    hdf5_filename_query = '_'.join(query_image.split('_')[:2]) + '.hdf5'\n",
    "    hdf5_file_query = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query), 'r')\n",
    "    \n",
    "    num_query = int(query_image.split('_')[-1])\n",
    "    \n",
    "    keypoints_query = keypoints_file[query_image]['keypoints'].__array__()\n",
    "    all_matched_keypoints_ids = np.full((len(keypoints_query)), False)\n",
    "    \n",
    "    for num in range(len(db_images)):\n",
    "        if num >= top_num_db_image:\n",
    "            break\n",
    "        db_image = db_images[num]\n",
    "        hdf5_filename_db = '_'.join(db_image.split('_')[:2]) + '.hdf5'\n",
    "        hdf5_file_db = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_db), 'r')\n",
    "        \n",
    "        num_db = int(db_image.split('_')[-1])\n",
    "        \n",
    "        depth_db = 10*hdf5_file_db['depth_base'][num_db]\n",
    "        \n",
    "        translation = hdf5_file_db['gps_base'][num_db]\n",
    "        orientation_matrix_44 = quaternion_to_rotation_matrix(hdf5_file_db['quat_base'][num_db])\n",
    "        pose_44 = np.eye(4)\n",
    "        pose_44[:3,:3] = orientation_matrix_44\n",
    "        pose_44[:3,3] = translation\n",
    "        \n",
    "        matches_db_keypoints_ids = match_file['_'.join([query_image,db_image])]['matches0'].__array__()\n",
    "        matched_keypoints_query_ids = np.where(matches_db_keypoints_ids > -1)[0]\n",
    "        for keypoint_id_query in matched_keypoints_query_ids:\n",
    "            if all_matched_keypoints_ids[keypoint_id_query] == False:\n",
    "                keypoint = keypoints_query[keypoint_id_query]\n",
    "                depth_keypoint = depth_db[int(keypoint[1]), int(keypoint[0])][0]\n",
    "                if depth_keypoint <= 0:\n",
    "                    continue\n",
    "                global_xyz_of_keypoint = get_point_3d(keypoint[0], keypoint[1], depth_keypoint, \\\n",
    "                                                      fx, fy, cx, cy, \\\n",
    "                                                      translation, hdf5_file_db['quat_base'][num_db])\n",
    "                keypoint += 0.5\n",
    "                keypoints_2D.append(list(keypoint))\n",
    "                keypoints_3D.append(list(global_xyz_of_keypoint))\n",
    "                all_matched_keypoints_ids[keypoint_id_query] = True           \n",
    "#     ret = pycolmap.absolute_pose_estimation(keypoints_2D, keypoints_3D, cfg, 12)\n",
    "#     print(ret['success'])\n",
    "#     print(ret['tvec'])\n",
    "#     print(yaw_pitch_roll_from_quaternion(list(ret['qvec']), w_first=True))\n",
    "#     break\n",
    "#     r = R.from_matrix(result.matrix()[:3,:3])\n",
    "#     yaw_pitch_roll = r.as_euler('yxz', degrees=True)\n",
    "    orientation_quat_wxyz = hdf5_file_db['quat_base'][num_db]\n",
    "#     results[query_image] = {\n",
    "#         'xyz': list(map(float,translation)),\n",
    "#         'quaternion_xyzw': list([float(orientation[1]), float(orientation[2]), float(orientation[3]), float(orientation[0])])\n",
    "#     }\n",
    "#     r = R.from_quat(list([orientation[1], orientation[2], orientation[3], orientation[0]]))\n",
    "#     yaw_pitch_roll = r.as_euler('yxz', degrees=True)\n",
    "#     print(query_image)\n",
    "#     print(\"NetVLAD\")\n",
    "#     print([orientation[1], orientation[2], orientation[3], orientation[0]])\n",
    "#     print(translation)\n",
    "#     print(yaw_pitch_roll)\n",
    "#     print(\"query\")\n",
    "\n",
    "    pose_estimated = np.eye(4)\n",
    "    pose_query = np.eye(4)\n",
    "    \n",
    "    r = R.from_quat([orientation_quat_wxyz[1], orientation_quat_wxyz[2], orientation_quat_wxyz[3], orientation_quat_wxyz[0]])\n",
    "#     print([orientation_quat_wxyz[1], orientation_quat_wxyz[2], orientation_quat_wxyz[3], orientation_quat_wxyz[0]])\n",
    "    pose_estimated[:3, :3] = r.as_matrix()\n",
    "    pose_estimated[:3, 3] = translation\n",
    "    \n",
    "    orientation_quat_xyzw = [hdf5_file_query['quat'][num_query][1], hdf5_file_query['quat'][num_query][2],\\\n",
    "                            hdf5_file_query['quat'][num_query][3], hdf5_file_query['quat'][num_query][0]]\n",
    "    r = R.from_quat(orientation_quat_xyzw)\n",
    "    pose_query[:3, :3] = r.as_matrix()\n",
    "    pose_query[:3, 3] = hdf5_file_query['gps'][num_query]\n",
    "    error_pose = np.linalg.inv(pose_estimated) @ pose_query\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = R.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / 3.14159265353\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    \n",
    "    if dist_error < 5: #and angle_error < 20 and dist_error < 5:\n",
    "        metrics[\"(5m, 20°)\"] += 1\n",
    "    if dist_error < 1: #and angle_error < 10:\n",
    "        metrics[\"(1m, 10°)\"] += 1\n",
    "    if dist_error < 0.5: #and angle_error < 5:\n",
    "        metrics[\"(0.5m, 5°)\"] += 1\n",
    "    if dist_error < 0.25: #and angle_error < 2:\n",
    "        metrics[\"(0.25m, 2°)\"] += 1\n",
    "for key in metrics.keys():\n",
    "    metrics[key] = metrics[key] / 950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(outputs / 'localization_results.json'), 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename_query = '_'.join(query_image.split('_')[:2]) + '.hdf5'\n",
    "hdf5_file_query = h5py.File(os.path.join(path_to_hdf5_datasets, hdf5_filename_query), 'r')\n",
    "num_query = int(query_image.split('_')[-1])\n",
    "translation_query = hdf5_file_query['gps'][num_query]\n",
    "orientation = quaternion_to_rotation_matrix(hdf5_file_query['quat'][num_query])\n",
    "r = R.from_matrix(orientation)\n",
    "yaw_pitch_roll = r.as_euler('yxz', degrees=True)\n",
    "print(translation_query)\n",
    "print(yaw_pitch_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
